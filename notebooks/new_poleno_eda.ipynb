{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ee2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --force-reinstall --no-cache-dir \"numpy<2\" scipy seaborn matplotlib pandas pyarrow\n",
    "\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from notebooks.notebook_helpers import load_train_and_test_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be02d9",
   "metadata": {},
   "source": [
    "## 1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1634f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "df = pd.read_csv(\"./data/final/poleno/poleno_labels_clean.csv\", index_col=0)\n",
    "\n",
    "# Train/test split\n",
    "train_ids, test_ids = load_train_and_test_ids(\"./data/raw/available_dataset_ids_swisens.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344d427",
   "metadata": {},
   "source": [
    "## 2 Pre-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af056a",
   "metadata": {},
   "source": [
    "### 2.1 Drop Invalid Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c53850",
   "metadata": {},
   "source": [
    "Drop old samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f7998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old samples\n",
    "keep_datasets = train_ids.loc[train_ids[\"throw-out\"]==False, \"dataset_id\"].unique()\n",
    "before_count = len(df)\n",
    "df = df[df[\"dataset_id\"].isin(keep_datasets)]\n",
    "print(f\"Removed {before_count - len(df)} rows from thrown-out datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18edfab",
   "metadata": {},
   "source": [
    "Drop samples with nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad842b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows = df[df.isnull().any(axis=1)]\n",
    "null_rows[\"dataset_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ed045",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_before_dropna = len(df)\n",
    "df = df.dropna()\n",
    "print(f\"Samples in original dataset: {len_before_dropna}\\nDropping samples with nan : {len_before_dropna-len(df)}\\nLength after dropping : {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f43f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"dataset_id\", \"species\", \"genus\"]\n",
    "\n",
    "for col in columns:\n",
    "    print(f\"Nr unique {col:<{max(len(c) for c in columns)}} : {len(df[col].value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878bf5c1",
   "metadata": {},
   "source": [
    "#### Number of images per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_event_id = df[\"event_id\"].value_counts()\n",
    "valid_samples_per_event_id = samples_per_event_id[samples_per_event_id == 2].index\n",
    "samples_per_event_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c0e15",
   "metadata": {},
   "source": [
    "Drop columns with more then two samples per `event_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"event_id\"].isin(valid_samples_per_event_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396058c",
   "metadata": {},
   "source": [
    "Drop columns that dont have exacly one image_nr 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastest way to count occurrences of 0/1 per event_id\n",
    "counts = (\n",
    "    df.groupby([\"event_id\", \"image_nr\"])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Keep event_ids with exactly one 0 and one 1\n",
    "valid_ids = counts[(counts[0] == 1) & (counts[1] == 1)].index\n",
    "\n",
    "len_before = len(df)\n",
    "df = df[df[\"event_id\"].isin(valid_ids)]\n",
    "print(len(df) - len_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_images_per_label(df, column=\"species\", min_count=2000, highlight_column=None, highlight_labels=None):\n",
    "\n",
    "#     highlight_column = column if highlight_column is None else highlight_column\n",
    "\n",
    "#     sample_columns = list(set([column, highlight_column ]))\n",
    "\n",
    "#     samples_per_label = df.value_counts(sample_columns).reset_index()\n",
    "\n",
    "#     # Create color list (default all blue)\n",
    "#     colors = [\"skyblue\"] * len(samples_per_label)\n",
    "\n",
    "\n",
    "#     if highlight_labels:\n",
    "#         highlight_set = set(highlight_labels)\n",
    "#         colors = [\n",
    "#             \"orange\" if label in highlight_set else \"skyblue\"\n",
    "#             for label in samples_per_label[highlight_column]\n",
    "#         ]\n",
    "\n",
    "#     plt.figure(figsize=(14, 4))\n",
    "#     plt.bar(x=samples_per_label[column], height=samples_per_label[\"count\"], color=colors)\n",
    "#     plt.axhline(min_count, color='red', ls='dotted')\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.title(f\"NR of samples for {column}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce46f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight_labels = [\"Acer\", \"Chenopodium\", \"Platanus\", \"Holcus\", \"Cynosurus\", \"Anthoxanthum\"]\n",
    "# show_images_per_label(df, column=\"genus\", min_count=10000, highlight_labels=highlight_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images_per_label(df, column=\"species\", min_count=2000, highlight_column=\"genus\", highlight_labels=highlight_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb058c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight_labels=[]\n",
    "# show_images_per_label(df, column=\"dataset_id\", min_count=2000, highlight_labels=highlight_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3e704",
   "metadata": {},
   "source": [
    "### 2.2 Create additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d338e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_common_root(df, root_col=\"root\"):\n",
    "    \"\"\"\n",
    "    Given a dataframe with a column containing absolute root paths,\n",
    "    returns:\n",
    "      - common_root: the longest shared prefix directory\n",
    "      - df with new column 'intermediate_path'\n",
    "    \"\"\"\n",
    "    roots = df[root_col].astype(str).tolist()\n",
    "\n",
    "    # Normalize and split all paths\n",
    "    split_paths = [os.path.normpath(r).split(os.sep) for r in roots]\n",
    "\n",
    "    # Find longest common prefix across all rows\n",
    "    common_parts = []\n",
    "    for parts in zip(*split_paths):\n",
    "        if all(p == parts[0] for p in parts):\n",
    "            common_parts.append(parts[0])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    common_root = os.sep.join(common_parts)\n",
    "\n",
    "    # Compute intermediate path for each row = root minus common prefix\n",
    "    common_len = len(common_parts)\n",
    "    df[\"intermediate_path\"] = [\n",
    "        os.sep.join(parts[common_len:]) if len(parts) > common_len else \"\"\n",
    "        for parts in split_paths\n",
    "    ]\n",
    "\n",
    "    return common_root, df\n",
    "\n",
    "\n",
    "def build_img_path(df, dataset_id_col=\"dataset_id\", rec_path_col=\"rec_path\"):\n",
    "    \"\"\"\n",
    "    Creates an img_path column by combining:\n",
    "        dataset_id / intermediate_path / rec_path\n",
    "    \"\"\"\n",
    "    df[\"img_path\"] = df.apply(\n",
    "        lambda row: os.path.join(\n",
    "            str(row[\"intermediate_path\"]),\n",
    "            str(row[dataset_id_col]),\n",
    "            str(row[rec_path_col])\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "_, df = extract_common_root(df, root_col=\"root\")\n",
    "df = build_img_path(df, dataset_id_col=\"dataset_id\", rec_path_col=\"rec_path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c49cb6",
   "metadata": {},
   "source": [
    "Force correct dataset to species mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfad329",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/Poleno_25/dataset_ids.json\" , \"r\") as f:\n",
    "    dataset_ids_to_species = json.load(f)\n",
    "\n",
    "df.loc[df[\"dataset_id\"].isin(dataset_ids_to_species), \"species\"] = \\\n",
    "    df.loc[df[\"dataset_id\"].isin(dataset_ids_to_species), \"dataset_id\"].map(dataset_ids_to_species)\n",
    "\n",
    "df[\"genus\"] = df[\"species\"].apply(lambda x: x.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bdf290",
   "metadata": {},
   "source": [
    "## 3 Dataset Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1043c49",
   "metadata": {},
   "source": [
    "### 3.1 Split into base, collection and zero dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba67500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, column, values):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into two parts based on matching values in a given column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame.\n",
    "    column : str\n",
    "        Column name to filter on (e.g., 'dataset_id', 'species', etc.).\n",
    "    values : list\n",
    "        List of values to select from the specified column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (df_selected, df_remaining)\n",
    "        df_selected  -> rows where df[column] is in values\n",
    "        df_remaining -> all other rows\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if column not in df.columns:\n",
    "        raise KeyError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "    if not isinstance(values, (list, tuple, set)):\n",
    "        raise TypeError(\"Parameter 'values' must be a list, tuple, or set.\")\n",
    "\n",
    "    mask = df[column].isin(values)\n",
    "    df_selected = df[mask].copy()\n",
    "    df_remaining = df[~mask].copy()\n",
    "\n",
    "    return df_selected, df_remaining\n",
    "\n",
    "\n",
    "# isolate_species = ['Urtica sp. 1', 'Urtica sp. 1', 'Juniperus communis', 'Lolium rigidum',]\n",
    "\n",
    "isolate_dataset = test_ids[\"dataset_id\"].tolist()\n",
    "\n",
    "# df_species_isolated, df_base = split_dataframe(df, column=\"species\", values=isolate_species)\n",
    "df_dataset_isolated, df_base = split_dataframe(df, column=\"dataset_id\", values=isolate_dataset)\n",
    "\n",
    "df_base = df_base.sort_values(by=\"event_id\")\n",
    "df_dataset_isolated = df_dataset_isolated.sort_values(by=\"event_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed5309",
   "metadata": {},
   "source": [
    "### 3.2 Split into train, val, test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d82a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test_grouped(\n",
    "    df,\n",
    "    stratify_col,\n",
    "    group_col=\"event_id\",\n",
    "    test_size_per_class=50,\n",
    "    val_size_per_class=20,\n",
    "    test_prop_per_class=0,\n",
    "    val_prop_per_class=0,   \n",
    "    random_state=42,\n",
    "):\n",
    "    df = df.copy()\n",
    "\n",
    "    if val_prop_per_class + test_prop_per_class >= 1.0:\n",
    "        raise ValueError(\"Sum of val_prop_per_class and test_prop_per_class must be less than 1.0\")\n",
    "\n",
    "    if stratify_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{stratify_col}' not found in DataFrame.\")\n",
    "    if group_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{group_col}' not found in DataFrame.\")\n",
    "\n",
    "    event_info = df[[group_col, stratify_col]].drop_duplicates(subset=group_col)\n",
    "    event_info = shuffle(event_info, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    test_events = []\n",
    "    val_events = []\n",
    "    train_events = []\n",
    "\n",
    "    for label, group in event_info.groupby(stratify_col):\n",
    "        total_groups = len(group)\n",
    "\n",
    "        # ---------------------\n",
    "        # TEST count\n",
    "        # ---------------------\n",
    "        prop_test = math.ceil(test_prop_per_class * total_groups)\n",
    "        n_test = max(test_size_per_class, prop_test)\n",
    "        n_test = min(n_test, total_groups)\n",
    "\n",
    "        test_part = group.sample(n=n_test, random_state=random_state)\n",
    "        remaining = group.drop(test_part.index)\n",
    "\n",
    "        # ---------------------\n",
    "        # VAL count\n",
    "        # ---------------------\n",
    "        prop_val = math.ceil(val_prop_per_class * total_groups)\n",
    "\n",
    "        n_val = max(val_size_per_class, prop_val)\n",
    "        n_val = min(n_val, len(remaining))  # still clamp to available groups\n",
    "\n",
    "        val_part = remaining.sample(n=n_val, random_state=random_state)\n",
    "        train_part = remaining.drop(val_part.index)\n",
    "\n",
    "        test_events.append(test_part)\n",
    "        val_events.append(val_part)\n",
    "        train_events.append(train_part)\n",
    "\n",
    "    test_events = pd.concat(test_events)\n",
    "    val_events = pd.concat(val_events)\n",
    "    train_events = pd.concat(train_events)\n",
    "\n",
    "    df_test  = df[df[group_col].isin(test_events[group_col])]\n",
    "    df_val   = df[df[group_col].isin(val_events[group_col])]\n",
    "    df_train = df[df[group_col].isin(train_events[group_col])]\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd837c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_config = {\n",
    "    \"name\": \"basic\",\n",
    "    \"data\": df_base,\n",
    "    \"val_prop_per_class\": 0.05,\n",
    "    \"min_val_events_per_class\": 20,\n",
    "    \"test_prop_per_class\": 0.1,\n",
    "    \"min_test_events_per_class\": 20,\n",
    "}\n",
    "\n",
    "# separate_species_config = {\n",
    "#     \"data\": df_species_isolated,\n",
    "#     \"val_prop_per_class\": 0.05,\n",
    "#     \"min_val_events_per_class\": 25,\n",
    "#     \"test_prop_per_class\": 0.1,\n",
    "#     \"min_test_events_per_class\": 25,\n",
    "# }\n",
    "\n",
    "separate_dataset_config = {\n",
    "    \"name\": \"isolated\",\n",
    "    \"data\": df_dataset_isolated,\n",
    "    \"val_prop_per_class\": 0.05,\n",
    "    \"min_val_events_per_class\": 20,\n",
    "    \"test_prop_per_class\": 0.1,\n",
    "    \"min_test_events_per_class\": 20,\n",
    "}\n",
    "\n",
    "split_configs = [basic_config, separate_dataset_config]\n",
    "\n",
    "save_in = \"data/final/poleno/\"\n",
    "save_csv = False\n",
    "\n",
    "for config in split_configs:\n",
    "\n",
    "    train_df, val_df, test_df = split_train_val_test_grouped(\n",
    "        df=config[\"data\"],\n",
    "        stratify_col=\"dataset_id\",\n",
    "        group_col=\"event_id\",\n",
    "        test_size_per_class=config[\"min_test_events_per_class\"],\n",
    "        test_prop_per_class=config[\"test_prop_per_class\"],\n",
    "        val_size_per_class=config[\"min_val_events_per_class\"],\n",
    "        val_prop_per_class=config[\"val_prop_per_class\"],\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    print(\"Train events:\", train_df[\"event_id\"].nunique())\n",
    "    print(\"Val events:\", val_df[\"event_id\"].nunique())\n",
    "    print(\"Test events:\", test_df[\"event_id\"].nunique())\n",
    "\n",
    "    if save_csv:\n",
    "        train_df.to_csv(os.path.join(save_in, config[\"name\"] + \"_train.csv\"), index=False)\n",
    "        val_df.to_csv(os.path.join(save_in, config[\"name\"] + \"_val.csv\"), index=False)\n",
    "        test_df.to_csv(os.path.join(save_in, config[\"name\"] + \"_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = r\"Z:\\marvel\\marvel-fhnw\\data\\Poleno25\"\n",
    "\n",
    "# # get parquet file to check\n",
    "# parquet_files = [f for f in os.listdir(r\"Z:\\marvel\\marvel-fhnw\\data\\Poleno25\") if f.endswith(\".parquet\")]\n",
    "\n",
    "# complete_id_to_genus = {}\n",
    "# complete_id_to_species = {}\n",
    "# for file in parquet_files:\n",
    "#     print(f\"Processing file: {file}\")\n",
    "#     parquet = pd.read_parquet(os.path.join(root, file))\n",
    "#     id_to_genus = dict(zip(parquet[\"dataset_id\"], parquet[\"genus\"]))\n",
    "#     id_to_species = dict(zip(parquet[\"dataset_id\"], parquet[\"species\"]))\n",
    "#     complete_id_to_genus.update(id_to_genus)\n",
    "#     complete_id_to_species.update(id_to_species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af8d7c",
   "metadata": {},
   "source": [
    "## Check species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_id = \"11edffad-46fc-1c2c-9d9c-66f2ec8a65cb\"\n",
    "# df.loc[df[\"dataset_id\"] == dataset_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str(pd.read_parquet(f\"Z:/marvel/marvel-fhnw/data/Poleno25/{dataset_id}.parquet\")[\"species\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4559d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_ids_to_species[dataset_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ed678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dataset_genus_mismatches(df, dataset_col, genus_col, mapping):\n",
    "    \"\"\"\n",
    "    Compare unique dataset_id → genus pairs in a DataFrame\n",
    "    with a dictionary mapping dataset_id → expected genus.\n",
    "\n",
    "    Prints mismatches.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract unique pairs\n",
    "    unique_pairs = df[[dataset_col, genus_col]].drop_duplicates()\n",
    "\n",
    "    mismatches = []\n",
    "\n",
    "    for _, row in unique_pairs.iterrows():\n",
    "        dataset = row[dataset_col]\n",
    "        genus = row[genus_col]\n",
    "\n",
    "        if dataset in mapping:\n",
    "            expected_genus = mapping[dataset]\n",
    "            if genus != expected_genus:\n",
    "                mismatches.append((dataset, genus, expected_genus))\n",
    "\n",
    "    # Print results\n",
    "    if not mismatches:\n",
    "        print(\"No mismatches found.\")\n",
    "    else:\n",
    "        print(\"Mismatches:\")\n",
    "        for ds, g, exp in mismatches:\n",
    "            print(f\"  dataset_id={ds} | genus={g} | expected={exp}\")\n",
    "\n",
    "    return mismatches\n",
    "\n",
    "mismatches = find_dataset_genus_mismatches(df, \"dataset_id\", \"species\", dataset_ids_to_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f409a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_species(df, dataset_col=\"dataset_id\", species_col=\"species\"):\n",
    "    \"\"\"\n",
    "    Returns dataset_ids that have more than one species.\n",
    "    \"\"\"\n",
    "    # Count how many times each (dataset_id, species) pair occurs\n",
    "    counts = df[[dataset_col, species_col]].drop_duplicates()\n",
    "\n",
    "    # Number of unique species per dataset_id\n",
    "    species_counts = counts[dataset_col].value_counts()\n",
    "\n",
    "    # IDs with >1 species\n",
    "    inconsistent = species_counts[species_counts > 1].index.tolist()\n",
    "\n",
    "    if not inconsistent:\n",
    "        print(\"ok ✅\")\n",
    "        return []\n",
    "    else:\n",
    "        print(\"Inconsistent dataset_ids:\")\n",
    "        for ds in inconsistent:\n",
    "            species_list = df[df[dataset_col] == ds][species_col].unique()\n",
    "            print(f\"  {ds} -> {species_list}\")\n",
    "\n",
    "    return inconsistent\n",
    "\n",
    "\n",
    "_ = check_dataset_species(df, dataset_col=\"dataset_id\", species_col=\"species\")\n",
    "\n",
    "# check_dataset_species(df, dataset_col=\"species\", species_col=\"dataset_id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "three_D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
