{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713ee2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --force-reinstall --no-cache-dir \"numpy<2\" scipy seaborn matplotlib pandas pyarrow\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "if os.path.basename(os.getcwd()) == \"notebooks\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from notebooks.notebook_helpers import load_train_and_test_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65be02d9",
   "metadata": {},
   "source": [
    "## 1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1634f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "df = pd.read_csv(\"./data/final/poleno/poleno_labels_clean.csv\", index_col=0)\n",
    "\n",
    "# Train/test split\n",
    "train_ids, test_ids = load_train_and_test_ids(\"./data/raw/available_dataset_ids_swisens.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344d427",
   "metadata": {},
   "source": [
    "## 2 Pre-Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83af056a",
   "metadata": {},
   "source": [
    "### 2.1 Drop Invalid Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c53850",
   "metadata": {},
   "source": [
    "Drop old samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e914f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_ids = train_ids.loc[train_ids[\"throw-out\"]==False, \"dataset_id\"].unique().tolist()\n",
    "test_dataset_ids = test_ids[\"dataset_id\"].tolist()\n",
    "ignore = []\n",
    "keep_datasets = list(set(train_dataset_ids + test_dataset_ids) - set(ignore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f7998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop old samples\n",
    "before_count = len(df)\n",
    "df = df[df[\"dataset_id\"].isin(keep_datasets)]\n",
    "print(f\"Removed {before_count - len(df)} rows from thrown-out datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18edfab",
   "metadata": {},
   "source": [
    "Drop samples with nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad842b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_rows = df[df.isnull().any(axis=1)]\n",
    "null_rows[\"dataset_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872ed045",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_before_dropna = len(df)\n",
    "df = df.dropna()\n",
    "print(f\"Samples in original dataset: {len_before_dropna}\\nDropping samples with nan : {len_before_dropna-len(df)}\\nLength after dropping : {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878bf5c1",
   "metadata": {},
   "source": [
    "#### Number of images per event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_event_id = df[\"event_id\"].value_counts()\n",
    "valid_samples_per_event_id = samples_per_event_id[samples_per_event_id == 2].index\n",
    "samples_per_event_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301c0e15",
   "metadata": {},
   "source": [
    "Drop columns with more then two samples per `event_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df[\"event_id\"].isin(valid_samples_per_event_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d396058c",
   "metadata": {},
   "source": [
    "Drop columns that dont have exacly one image_nr 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad1a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fastest way to count occurrences of 0/1 per event_id\n",
    "counts = (\n",
    "    df.groupby([\"event_id\", \"image_nr\"])\n",
    "      .size()\n",
    "      .unstack(fill_value=0)\n",
    ")\n",
    "\n",
    "# Keep event_ids with exactly one 0 and one 1\n",
    "valid_ids = counts[(counts[0] == 1) & (counts[1] == 1)].index\n",
    "\n",
    "len_before = len(df)\n",
    "df = df[df[\"event_id\"].isin(valid_ids)]\n",
    "print(len(df) - len_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_images_per_label(df, column=\"species\", min_count=2000, highlight_column=None, highlight_labels=None):\n",
    "\n",
    "#     highlight_column = column if highlight_column is None else highlight_column\n",
    "\n",
    "#     sample_columns = list(set([column, highlight_column ]))\n",
    "\n",
    "#     samples_per_label = df.value_counts(sample_columns).reset_index()\n",
    "\n",
    "#     # Create color list (default all blue)\n",
    "#     colors = [\"skyblue\"] * len(samples_per_label)\n",
    "\n",
    "\n",
    "#     if highlight_labels:\n",
    "#         highlight_set = set(highlight_labels)\n",
    "#         colors = [\n",
    "#             \"orange\" if label in highlight_set else \"skyblue\"\n",
    "#             for label in samples_per_label[highlight_column]\n",
    "#         ]\n",
    "\n",
    "#     plt.figure(figsize=(14, 4))\n",
    "#     plt.bar(x=samples_per_label[column], height=samples_per_label[\"count\"], color=colors)\n",
    "#     plt.axhline(min_count, color='red', ls='dotted')\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.title(f\"NR of samples for {column}\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce46f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight_labels = [\"Acer\", \"Chenopodium\", \"Platanus\", \"Holcus\", \"Cynosurus\", \"Anthoxanthum\"]\n",
    "# show_images_per_label(df, column=\"genus\", min_count=10000, highlight_labels=highlight_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879d93d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_images_per_label(df, column=\"species\", min_count=2000, highlight_column=\"genus\", highlight_labels=highlight_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb058c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# highlight_labels=[]\n",
    "# show_images_per_label(df, column=\"dataset_id\", min_count=2000, highlight_labels=highlight_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cbeea7",
   "metadata": {},
   "source": [
    "Correct root and intermediate path for testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec26f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"dataset_id\"].isin(test_dataset_ids), \"root\"] = \"Z:/marvel/marvel-fhnw/data/Poleno25_test\"\n",
    "df.loc[df[\"dataset_id\"].isin(test_dataset_ids), \"intermediate_path\"] = \"Poleno25_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b3e704",
   "metadata": {},
   "source": [
    "### 2.2 Create additional columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d338e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_common_root(df, root_col=\"root\"):\n",
    "    \"\"\"\n",
    "    Given a dataframe with a column containing absolute root paths,\n",
    "    returns:\n",
    "      - common_root: the longest shared prefix directory\n",
    "      - df with new column 'intermediate_path'\n",
    "    \"\"\"\n",
    "    roots = df[root_col].astype(str).tolist()\n",
    "\n",
    "    # Normalize and split all paths\n",
    "    split_paths = [os.path.normpath(r).split(os.sep) for r in roots]\n",
    "\n",
    "    # Find longest common prefix across all rows\n",
    "    common_parts = []\n",
    "    for parts in zip(*split_paths):\n",
    "        if all(p == parts[0] for p in parts):\n",
    "            common_parts.append(parts[0])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    common_root = os.sep.join(common_parts)\n",
    "\n",
    "    # Compute intermediate path for each row = root minus common prefix\n",
    "    common_len = len(common_parts)\n",
    "    df[\"intermediate_path\"] = [\n",
    "        os.sep.join(parts[common_len:]) if len(parts) > common_len else \"\"\n",
    "        for parts in split_paths\n",
    "    ]\n",
    "\n",
    "    return common_root, df\n",
    "\n",
    "\n",
    "def build_img_path(df, dataset_id_col=\"dataset_id\", rec_path_col=\"rec_path\"):\n",
    "    \"\"\"\n",
    "    Creates an img_path column by combining:\n",
    "        dataset_id / intermediate_path / rec_path\n",
    "    \"\"\"\n",
    "    df[\"img_path\"] = df.apply(\n",
    "        lambda row: os.path.join(\n",
    "            str(row[\"intermediate_path\"]),\n",
    "            str(row[dataset_id_col]),\n",
    "            str(row[rec_path_col])\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    df[\"img_path\"] = df[\"img_path\"].str.replace(\"\\\\\", \"/\")\n",
    "    return df\n",
    "\n",
    "\n",
    "_, df = extract_common_root(df, root_col=\"root\")\n",
    "df = build_img_path(df, dataset_id_col=\"dataset_id\", rec_path_col=\"rec_path\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c49cb6",
   "metadata": {},
   "source": [
    "### Force correct dataset to species mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfad329",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed/Poleno_25/dataset_ids.json\" , \"r\") as f:\n",
    "    dataset_ids_to_species = json.load(f)\n",
    "\n",
    "df.loc[df[\"dataset_id\"].isin(dataset_ids_to_species), \"species\"] = \\\n",
    "    df.loc[df[\"dataset_id\"].isin(dataset_ids_to_species), \"dataset_id\"].map(dataset_ids_to_species)\n",
    "\n",
    "df[\"genus\"] = df[\"species\"].apply(lambda x: x.split()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed9c896",
   "metadata": {},
   "source": [
    "### Norm species name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2922904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def normalize_species(label: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize species labels like:\n",
    "      'Urtica sp'\n",
    "      'Urtica sp1'\n",
    "      'Urtica sp 1'\n",
    "      'Urtica sp.2'\n",
    "      'Urtica sp. 2'\n",
    "    → 'Urtica sp.'\n",
    "    \"\"\"\n",
    "    label = label.strip()\n",
    "    match = re.match(\n",
    "        r'^(\\w+)\\s+sp\\.?\\s*\\d*$',\n",
    "        label,\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    if match:\n",
    "        genus = match.group(1)\n",
    "        return f\"{genus} sp.\"\n",
    "\n",
    "    return label\n",
    "\n",
    "df[\"species_norm\"] = df[\"species\"].apply(normalize_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c4273",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"dataset_id\", \"species\", \"genus\"]\n",
    "\n",
    "for col in columns:\n",
    "    print(f\"Nr unique {col:<{max(len(c) for c in columns)}} : {len(df[col].value_counts())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6d4688",
   "metadata": {},
   "source": [
    "### Add enumerated labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a669f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_per_distinct_label(labels):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        labels: iterable of labels (e.g. list or 1D tensor)\n",
    "\n",
    "    Returns:\n",
    "        indices: list of ints (same length as labels)\n",
    "        label_to_index: dict mapping label -> index\n",
    "    \"\"\"\n",
    "    label_to_index = {}\n",
    "    indices = []\n",
    "    next_idx = 0\n",
    "\n",
    "    for label in labels:\n",
    "        if label not in label_to_index:\n",
    "            label_to_index[label] = next_idx\n",
    "            next_idx += 1\n",
    "        indices.append(label_to_index[label])\n",
    "\n",
    "    return indices, label_to_index\n",
    "\n",
    "\n",
    "all_indices = {}\n",
    "for id_col in [\"dataset_id\", \"species_norm\", \"genus\"]:\n",
    "    df[id_col + \"_enum\"], mapping = index_per_distinct_label(df[id_col])\n",
    "    all_indices[id_col] = mapping\n",
    "\n",
    "with open(\"data/final/poleno/label_mappings.json\", \"w\") as f:\n",
    "    json.dump(all_indices, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bdf290",
   "metadata": {},
   "source": [
    "## 3 Dataset Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1043c49",
   "metadata": {},
   "source": [
    "### 3.1 Split into base, collection and zero dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba67500d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, column, values):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into two parts based on matching values in a given column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        The input DataFrame.\n",
    "    column : str\n",
    "        Column name to filter on (e.g., 'dataset_id', 'species', etc.).\n",
    "    values : list\n",
    "        List of values to select from the specified column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (df_selected, df_remaining)\n",
    "        df_selected  -> rows where df[column] is in values\n",
    "        df_remaining -> all other rows\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if column not in df.columns:\n",
    "        raise KeyError(f\"Column '{column}' not found in DataFrame.\")\n",
    "\n",
    "    if not isinstance(values, (list, tuple, set)):\n",
    "        raise TypeError(\"Parameter 'values' must be a list, tuple, or set.\")\n",
    "\n",
    "    mask = df[column].isin(values)\n",
    "    df_selected = df[mask].copy()\n",
    "    df_remaining = df[~mask].copy()\n",
    "\n",
    "    return df_selected, df_remaining\n",
    "\n",
    "\n",
    "# isolate_species = ['Urtica sp. 1', 'Urtica sp. 1', 'Juniperus communis', 'Lolium rigidum',]\n",
    "\n",
    "isolate_dataset = test_ids[\"dataset_id\"].tolist()\n",
    "\n",
    "# df_species_isolated, df_base = split_dataframe(df, column=\"species\", values=isolate_species)\n",
    "df_dataset_isolated, df_base = split_dataframe(df, column=\"dataset_id\", values=isolate_dataset)\n",
    "\n",
    "df_base = df_base.sort_values(by=\"event_id\")\n",
    "df_dataset_isolated = df_dataset_isolated.sort_values(by=\"event_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38066630",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_table = (\n",
    "    df.groupby(\"genus\")\n",
    "      .value_counts([\"species_norm\"])\n",
    "      .to_frame(\"count\")  # <-- stays MultiIndex, becomes DataFrame\n",
    ")\n",
    "\n",
    "latex_table\n",
    "\n",
    "latex_table1 = latex_table[0:41]\n",
    "latex_table2 = latex_table[41:]\n",
    "\n",
    "# print(latex_table1.to_latex())\n",
    "# print(latex_table2.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22286ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_images = latex_table[\"count\"].sum()\n",
    "total_events = int(total_images/2)\n",
    "total_species = latex_table.reset_index().value_counts(\"species_norm\").count()\n",
    "total_genus = latex_table.reset_index().value_counts(\"genus\").count()\n",
    "print(f\"Total: \\nimages: {total_images}\\nevents: {total_events}\\nspecies: {total_species}\\ngenus: {total_genus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ed5309",
   "metadata": {},
   "source": [
    "### 3.2 Split into train, val, test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d82a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val_test_grouped(\n",
    "    df,\n",
    "    stratify_col,\n",
    "    group_col=\"event_id\",\n",
    "    test_size_per_class=50,\n",
    "    val_size_per_class=20,\n",
    "    test_prop_per_class=0,\n",
    "    val_prop_per_class=0,   \n",
    "    random_state=42,\n",
    "):\n",
    "    df = df.copy()\n",
    "\n",
    "    if val_prop_per_class + test_prop_per_class >= 1.0:\n",
    "        raise ValueError(\"Sum of val_prop_per_class and test_prop_per_class must be less than 1.0\")\n",
    "\n",
    "    if stratify_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{stratify_col}' not found in DataFrame.\")\n",
    "    if group_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{group_col}' not found in DataFrame.\")\n",
    "\n",
    "    event_info = df[[group_col, stratify_col]].drop_duplicates(subset=group_col)\n",
    "    event_info = shuffle(event_info, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "    test_events = []\n",
    "    val_events = []\n",
    "    train_events = []\n",
    "\n",
    "    for label, group in event_info.groupby(stratify_col):\n",
    "        total_groups = len(group)\n",
    "\n",
    "        # ---------------------\n",
    "        # TEST count\n",
    "        # ---------------------\n",
    "        prop_test = math.ceil(test_prop_per_class * total_groups)\n",
    "        n_test = max(test_size_per_class, prop_test)\n",
    "        n_test = min(n_test, total_groups)\n",
    "\n",
    "        test_part = group.sample(n=n_test, random_state=random_state)\n",
    "        remaining = group.drop(test_part.index)\n",
    "\n",
    "        # ---------------------\n",
    "        # VAL count\n",
    "        # ---------------------\n",
    "        prop_val = math.ceil(val_prop_per_class * total_groups)\n",
    "\n",
    "        n_val = max(val_size_per_class, prop_val)\n",
    "        n_val = min(n_val, len(remaining))  # still clamp to available groups\n",
    "\n",
    "        val_part = remaining.sample(n=n_val, random_state=random_state)\n",
    "        train_part = remaining.drop(val_part.index)\n",
    "\n",
    "        test_events.append(test_part)\n",
    "        val_events.append(val_part)\n",
    "        train_events.append(train_part)\n",
    "\n",
    "    test_events = pd.concat(test_events)\n",
    "    val_events = pd.concat(val_events)\n",
    "    train_events = pd.concat(train_events)\n",
    "\n",
    "    df_test  = df[df[group_col].isin(test_events[group_col])]\n",
    "    df_val   = df[df[group_col].isin(val_events[group_col])]\n",
    "    df_train = df[df[group_col].isin(train_events[group_col])]\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f75a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_config = {\n",
    "    \"name\": \"basic\",\n",
    "    \"data\": df_base,\n",
    "    \"val_prop_per_class\": 0.05,\n",
    "    \"min_val_events_per_class\": 20,\n",
    "    \"test_prop_per_class\": 0.1,\n",
    "    \"min_test_events_per_class\": 20,\n",
    "}\n",
    "\n",
    "# separate_species_config = {\n",
    "#     \"data\": df_species_isolated,\n",
    "#     \"val_prop_per_class\": 0.05,\n",
    "#     \"min_val_events_per_class\": 25,\n",
    "#     \"test_prop_per_class\": 0.1,\n",
    "#     \"min_test_events_per_class\": 25,\n",
    "# }\n",
    "\n",
    "separate_dataset_config = {\n",
    "    \"name\": \"isolated\",\n",
    "    \"data\": df_dataset_isolated,\n",
    "    \"val_prop_per_class\": 0.05,\n",
    "    \"min_val_events_per_class\": 20,\n",
    "    \"test_prop_per_class\": 0.1,\n",
    "    \"min_test_events_per_class\": 20,\n",
    "}\n",
    "\n",
    "split_configs = [basic_config, separate_dataset_config]\n",
    "\n",
    "save_in = \"data/final/poleno/\"\n",
    "save_csv = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd837c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in split_configs:\n",
    "\n",
    "    print(config[\"data\"].shape)\n",
    "\n",
    "    train_df, val_df, test_df = split_train_val_test_grouped(\n",
    "        df=config[\"data\"],\n",
    "        stratify_col=\"dataset_id\",\n",
    "        group_col=\"event_id\",\n",
    "        test_size_per_class=config[\"min_test_events_per_class\"],\n",
    "        test_prop_per_class=config[\"test_prop_per_class\"],\n",
    "        val_size_per_class=config[\"min_val_events_per_class\"],\n",
    "        val_prop_per_class=config[\"val_prop_per_class\"],\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    print(config[\"name\"], \"total:\", config[\"data\"][\"event_id\"].nunique())\n",
    "    print(\"Train events:\", train_df[\"event_id\"].nunique())\n",
    "    print(\"Val events:\", val_df[\"event_id\"].nunique())\n",
    "    print(\"Test events:\", test_df[\"event_id\"].nunique())\n",
    "\n",
    "    if save_csv:\n",
    "        # comparison = pd.read_csv(os.path.join(save_in, config[\"name\"] + \"_train.csv\"))\n",
    "        # comparison2 = train_df\n",
    "        # cols = comparison.columns\n",
    "        # print(comparison2[cols].reset_index(drop=True).equals(comparison[cols].reset_index(drop=True)))\n",
    "        # print(comparison.shape, comparison2.shape)\n",
    "\n",
    "        # diff_counts = {}\n",
    "        # for col in comparison.columns:\n",
    "        #     s1 = comparison[col].reset_index(drop=True)\n",
    "        #     s2 = comparison2[col].reset_index(drop=True)\n",
    "        #     diff_counts[col] = (s1 != s2).sum()\n",
    "        # diff_counts = {k: v for k, v in diff_counts.items() if v > 0}\n",
    "        # print(diff_counts)\n",
    "\n",
    "        config[\"data\"].to_csv(os.path.join(save_in, config[\"name\"] + \"_all.csv\"), index=False)\n",
    "        train_df.to_csv(os.path.join(save_in, config[\"name\"] + \"_train.csv\"), index=False)\n",
    "        val_df.to_csv(os.path.join(save_in, config[\"name\"] + \"_val.csv\"), index=False)\n",
    "        test_df.to_csv(os.path.join(save_in, config[\"name\"] + \"_test.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdfbd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new = pd.read_csv(\"data/final/poleno/poleno_labels_clean.csv\")\n",
    "# old = pd.read_csv(\"data/final/poleno/poleno_labels_clean_old.csv\")\n",
    "old = pd.read_csv(\"data/final/poleno/archive/basic_val.csv\")\n",
    "new = pd.read_csv(\"data/final/poleno/basic_val.csv\")\n",
    "# # imtermediate = pd.read_csv(r\"C:\\Users\\simon\\Downloads\\collection_2025-11-16_16-35-31.csv\")\n",
    "testcols = old.columns\n",
    "(new[testcols].values == old[testcols].values).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de51d88",
   "metadata": {},
   "source": [
    "### Sub-Sample stratified validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624eeb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_events_per_dataset(df, n, dataset_col=\"dataset_id\", event_col=\"event_id\", random_state=None):\n",
    "    \"\"\"\n",
    "    Sample n events per dataset_id while keeping all rows of an event together.\n",
    "    \n",
    "    df: pandas DataFrame\n",
    "    n: number of events to sample per dataset\n",
    "    dataset_col: column that identifies dataset\n",
    "    event_col: column that identifies an event\n",
    "    random_state: for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        sampled_df: rows belonging to sampled events\n",
    "        remaining_df: all other rows\n",
    "    \"\"\"\n",
    "    \n",
    "    sampled_event_ids = []\n",
    "    \n",
    "    # Group by dataset_id\n",
    "    grouped = df.groupby(dataset_col)\n",
    "    \n",
    "    for dataset_id, subset in grouped:\n",
    "        # Unique events in this dataset_id\n",
    "        events = subset[event_col].unique()\n",
    "        \n",
    "        # If a dataset has fewer than n events, sample all\n",
    "        k = min(n, len(events))\n",
    "        \n",
    "        sampled = pd.Series(events).sample(k, random_state=random_state)\n",
    "        sampled_event_ids.extend(sampled.tolist())\n",
    "    \n",
    "    # Create boolean mask for sampled events\n",
    "    mask = df[event_col].isin(sampled_event_ids)\n",
    "    \n",
    "    sampled_df = df[mask].copy()\n",
    "    remaining_df = df[~mask].copy()\n",
    "    \n",
    "    return sampled_df, remaining_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df1e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_n = 20\n",
    "for config in split_configs:\n",
    "    df_val = pd.read_csv(os.path.join(save_in, config[\"name\"] + \"_val.csv\"))\n",
    "    \n",
    "    df_val_sampled, _ = sample_events_per_dataset(df_val, n=select_n, random_state=42)\n",
    "    print(df_val_sampled[\"event_id\"].nunique())\n",
    "    \n",
    "    df_val_sampled.to_csv(os.path.join(save_in, config[\"name\"] + f'_val_{select_n}.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febce598",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_n = 20\n",
    "for config in split_configs:\n",
    "    df_test = pd.read_csv(os.path.join(save_in, config[\"name\"] + \"_test.csv\"))\n",
    "    \n",
    "    df_test_sampled, _ = sample_events_per_dataset(df_test, n=select_n, random_state=42)\n",
    "    print(df_test_sampled[\"event_id\"].nunique())\n",
    "\n",
    "    df_test_sampled.to_csv(os.path.join(save_in, config[\"name\"] + f'_test_{select_n}.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84d676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# root = r\"Z:\\marvel\\marvel-fhnw\\data\\Poleno25\"\n",
    "\n",
    "# # get parquet file to check\n",
    "# parquet_files = [f for f in os.listdir(r\"Z:\\marvel\\marvel-fhnw\\data\\Poleno25\") if f.endswith(\".parquet\")]\n",
    "\n",
    "# complete_id_to_genus = {}\n",
    "# complete_id_to_species = {}\n",
    "# for file in parquet_files:\n",
    "#     print(f\"Processing file: {file}\")\n",
    "#     parquet = pd.read_parquet(os.path.join(root, file))\n",
    "#     id_to_genus = dict(zip(parquet[\"dataset_id\"], parquet[\"genus\"]))\n",
    "#     id_to_species = dict(zip(parquet[\"dataset_id\"], parquet[\"species\"]))\n",
    "#     complete_id_to_genus.update(id_to_genus)\n",
    "#     complete_id_to_species.update(id_to_species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07af8d7c",
   "metadata": {},
   "source": [
    "## Check species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6b6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_id = \"11edffad-46fc-1c2c-9d9c-66f2ec8a65cb\"\n",
    "# df.loc[df[\"dataset_id\"] == dataset_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a08e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str(pd.read_parquet(f\"Z:/marvel/marvel-fhnw/data/Poleno25/{dataset_id}.parquet\")[\"species\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4559d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_ids_to_species[dataset_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3ed678",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dataset_genus_mismatches(df, dataset_col, genus_col, mapping):\n",
    "    \"\"\"\n",
    "    Compare unique dataset_id → genus pairs in a DataFrame\n",
    "    with a dictionary mapping dataset_id → expected genus.\n",
    "\n",
    "    Prints mismatches.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract unique pairs\n",
    "    unique_pairs = df[[dataset_col, genus_col]].drop_duplicates()\n",
    "\n",
    "    mismatches = []\n",
    "\n",
    "    for _, row in unique_pairs.iterrows():\n",
    "        dataset = row[dataset_col]\n",
    "        genus = row[genus_col]\n",
    "\n",
    "        if dataset in mapping:\n",
    "            expected_genus = mapping[dataset]\n",
    "            if genus != expected_genus:\n",
    "                mismatches.append((dataset, genus, expected_genus))\n",
    "\n",
    "    # Print results\n",
    "    if not mismatches:\n",
    "        print(\"No mismatches found.\")\n",
    "    else:\n",
    "        print(\"Mismatches:\")\n",
    "        for ds, g, exp in mismatches:\n",
    "            print(f\"  dataset_id={ds} | genus={g} | expected={exp}\")\n",
    "\n",
    "    return mismatches\n",
    "\n",
    "mismatches = find_dataset_genus_mismatches(df, \"dataset_id\", \"species\", dataset_ids_to_species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f409a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dataset_species(df, dataset_col=\"dataset_id\", species_col=\"species\"):\n",
    "    \"\"\"\n",
    "    Returns dataset_ids that have more than one species.\n",
    "    \"\"\"\n",
    "    # Count how many times each (dataset_id, species) pair occurs\n",
    "    counts = df[[dataset_col, species_col]].drop_duplicates()\n",
    "\n",
    "    # Number of unique species per dataset_id\n",
    "    species_counts = counts[dataset_col].value_counts()\n",
    "\n",
    "    # IDs with >1 species\n",
    "    inconsistent = species_counts[species_counts > 1].index.tolist()\n",
    "\n",
    "    if not inconsistent:\n",
    "        print(\"ok ✅\")\n",
    "        return []\n",
    "    else:\n",
    "        print(\"Inconsistent dataset_ids:\")\n",
    "        for ds in inconsistent:\n",
    "            species_list = df[df[dataset_col] == ds][species_col].unique()\n",
    "            print(f\"  {ds} -> {species_list}\")\n",
    "\n",
    "    return inconsistent\n",
    "\n",
    "\n",
    "_ = check_dataset_species(df, dataset_col=\"dataset_id\", species_col=\"species\")\n",
    "\n",
    "# check_dataset_species(df, dataset_col=\"species\", species_col=\"dataset_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fce7d60",
   "metadata": {},
   "source": [
    "### Sample Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3040df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in split_configs:\n",
    "\n",
    "    train_df = pd.read_csv(os.path.join(save_in, config[\"name\"] + \"_train.csv\"))\n",
    "    val_df = pd.read_csv(os.path.join(save_in, config[\"name\"] + \"_val.csv\"))\n",
    "    test_df = pd.read_csv(os.path.join(save_in, config[\"name\"] + \"_test.csv\"))\n",
    "\n",
    "    print(config[\"name\"])\n",
    "    print(\"Train events:\", train_df[\"event_id\"].nunique())\n",
    "    print(\"Val events:\", val_df[\"event_id\"].nunique())\n",
    "    print(\"Test events:\", test_df[\"event_id\"].nunique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "three_D",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
